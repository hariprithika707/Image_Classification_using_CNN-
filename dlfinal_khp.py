# -*- coding: utf-8 -*-
"""DLfinal_KHP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oiqTPhPdWhhhluyhn4M4g7AR9PspPRxT

# **NIKE SHOES CLASSIFICATION USING CNN**

*IMPORT NECESSARY LIBRARIES*
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import os
import matplotlib.pyplot as plt
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

!pip install datasets

from datasets import load_dataset

"""*LODING DATASET*"""

dataset = load_dataset("HZhang729/Nike_Shoes_Classification")

# Save images to directories
def save_images(dataset, split, folder):
    os.makedirs(folder, exist_ok=True)
    for i, example in enumerate(dataset[split]):
        image = example['image']
        label = example['label']
        label_folder = os.path.join(folder, str(label))
        os.makedirs(label_folder, exist_ok=True)
        # Convert image to RGB mode before saving
        image = image.convert('RGB') #This line was added to convert the image to RGB mode
        image.save(os.path.join(label_folder, f'image_{i}.jpg'))

# Access the splits using the correct keys
save_images(dataset, 'train', 'nikeShoes/train')

"""*Split vadidation data and test data from train data*"""

import os
from sklearn.model_selection import train_test_split
import shutil

def create_directory_structure(base_dir, splits):
    for split in splits:
        split_dir = os.path.join(base_dir, split)
        os.makedirs(split_dir, exist_ok=True)
        for label in os.listdir(os.path.join(base_dir, 'train')):
            os.makedirs(os.path.join(split_dir, label), exist_ok=True)

def split_data(train_dir, val_split=0.19, test_split=0.11):
    for label in os.listdir(train_dir):
        label_dir = os.path.join(train_dir, label)
        images = os.listdir(label_dir)

        # Split data into training + validation and test sets
        train_val_images, test_images = train_test_split(images, test_size=test_split, random_state=42)

        # Split the remaining training + validation into training and validation sets
        train_images, val_images = train_test_split(train_val_images, test_size=val_split / (1 - test_split), random_state=42)

        # Define directories
        val_label_dir = os.path.join('nikeShoes/valid', label)
        test_label_dir = os.path.join('nikeShoes/test', label)

        # Move images to validation and test directories
        for image in val_images:
            shutil.move(os.path.join(label_dir, image), os.path.join(val_label_dir, image))

        for image in test_images:
            shutil.move(os.path.join(label_dir, image), os.path.join(test_label_dir, image))

def main():
    train_dir = 'nikeShoes/train/'
    create_directory_structure('nikeShoes', ['valid', 'test'])
    split_data(train_dir)

if __name__ == "__main__":
    main()

"""*PREPROCESSING*"""

import tensorflow as tf
from tensorflow.keras.preprocessing import image_dataset_from_directory

train_dataset = image_dataset_from_directory(
 'nikeShoes/train',
 image_size=(180, 180),
 batch_size=32)

validation_dataset = image_dataset_from_directory(
 'nikeShoes/valid',
 image_size=(180, 180),
 batch_size=32)

test_dataset = image_dataset_from_directory(
 'nikeShoes/test',
 image_size=(180, 180),
 batch_size=32)

#print the name class names

print(train_dataset.class_names)

class_names = {
    'Nike Air Forces': 0,
    'Nike Air Jordans': 1,
    'Nike Air Maxes': 2,
    'Nike Cleats': 3,
    'Nike Dunks': 4
}

import tensorflow as tf
from tensorflow.keras.preprocessing import image_dataset_from_directory
import matplotlib.pyplot as plt # Import matplotlib

plt.figure(figsize=(10,10))
# Iterate over the dataset and take only the first 25 images and labels
for images, labels in train_dataset.take(1): # Take only one batch from the dataset
    for i in range(25): # Iterate over the number of images in the batch
        plt.subplot(5, 5, i + 1)
        plt.xticks([])
        plt.yticks([])
        plt.grid(False)
        # Display the image
        plt.imshow(images[i].numpy().astype("uint8"))
        # Display the label
        # Convert the label tensor to a NumPy integer
        label_index = labels[i].numpy()
        # Use the integer index to access the class name
        plt.xlabel(list(class_names.keys())[list(class_names.values()).index(label_index)])
plt.show()

"""*TRAIN A CNN MODEL*"""

from tensorflow.keras import layers, models

# Define the CNN model
model = models.Sequential([
    layers.Rescaling(1./255, input_shape=(180, 180, 3)),  # Normalize pixel values
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(5, activation='softmax')  # Output layer for 4 classes
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.summary()

history = model.fit(train_dataset, epochs=10,
                    validation_data=validation_dataset)

# Evaluate the model on the test dataset
test_loss, test_acc = model.evaluate(test_dataset)
print(f"Test accuracy: {test_acc:.3f}")

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')

test_loss, test_acc = model.evaluate(test_dataset, verbose=2)

"""***RESULT:****The model performed well. But there is a chance of  **Overfitting** .....
*We perform regularization technique:* ***Dropout***
"""

model1 = models.Sequential([
    layers.Rescaling(1./255, input_shape=(180, 180, 3)),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(5, activation='softmax')
])

model1.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model1.summary()

history = model1.fit(train_dataset, epochs=10,
                    validation_data=validation_dataset)

# Evaluate the model on the test dataset
test_loss, test_acc = model1.evaluate(test_dataset)
print(f"Test accuracy: {test_acc:.3f}")

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')

test_loss, test_acc = model1.evaluate(test_dataset, verbose=2)

#saving the model
model.save('nikeShoes_classification.h5')

from google.colab import files
files.download('nikeShoes_classification.h5')

